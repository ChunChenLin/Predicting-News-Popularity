{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "def tokenizer_by_tense(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    important_words = []\n",
    "    for t in tagged:\n",
    "        tag_name = t[1][0:2]\n",
    "        word = t[0].lower()\n",
    "        if tag_name=='NN' or tag_name=='JJ' or tag_name=='VB' or tag_name=='RB':\n",
    "            porter = PorterStemmer()\n",
    "            important_words.append(word)\n",
    "        \n",
    "    return [porter.stem(w) for w in important_words if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "df_train = pd.read_csv('datasets/train.csv')\n",
    "df_test = pd.read_csv('datasets/test.csv')\n",
    "#print(df_train.head(5))\n",
    "#print(df_test.head(5))\n",
    "page0 = df_train.loc[0,'Page content']\n",
    "#print(page0)\n",
    "df_small = df_train.sample(n=50,random_state=0)\n",
    "#print(df_small.iloc[0]['Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse datetime from html\n",
    "from dateutil import parser\n",
    "\n",
    "# Extract features from raw HTML\n",
    "# Usage: \n",
    "# page0 = df.loc[0,'Page content']\n",
    "# extract_feature(page0)\n",
    "def extract_feature(html):\n",
    "    \n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # link\n",
    "    link_count = len(bs.findAll('a', href=True)) \n",
    "    \n",
    "    # image\n",
    "    img_count = len(bs.findAll('img'))\n",
    "    \n",
    "    # animation, chart or video\n",
    "    iframe_count = len(bs.findAll('iframe'))\n",
    "    \n",
    "    # quote\n",
    "    quote_count = len(bs.findAll('blockquote'))\n",
    "    \n",
    "    # tags\n",
    "    tags = [] \n",
    "    for tag in bs.select('footer a'):\n",
    "        tags.append(tag.string)\n",
    "        \n",
    "    # category\n",
    "    category = \"\"\n",
    "    for cat in bs.findAll('article'):\n",
    "        try:\n",
    "            category = cat['data-channel']\n",
    "            break\n",
    "        except:\n",
    "            category = \"\"\n",
    "            continue\n",
    "            \n",
    "    # author (seem not notable) -> discard(?)\n",
    "    '''\n",
    "    author_raw = bs.find(\"div\", { \"class\" : \"article-info\" })\n",
    "    try:\n",
    "        author = author_raw.find('a')['href']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        author = author_raw.find('span',{'class':'author_name'}).string\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "    \n",
    "    # title info. (h1) (must be helpful)\n",
    "    title = bs.find(\"h1\", { \"class\" : \"title\" }).string\n",
    "    title_words_count = len(re.split(r'\\s+', title))\n",
    "    title_digits_count = len([int(s) for s in title.split() if s.isdigit()])\n",
    "    title_question_mark = ('?' in title) # boolean\n",
    "    title_exclamation_mark = ('!' in title) # boolean\n",
    "    \n",
    "    # sub-title (h2) (must be helpful)\n",
    "    # count how many sub-title in the article\n",
    "    try:\n",
    "        h2 = bs.find('h2')\n",
    "        sub_title_count = len(bs.find('h2'))\n",
    "    except:\n",
    "        sub_title_count = 0\n",
    "    \n",
    "    # get word set\n",
    "    pre_text = preprocessor(html)\n",
    "    tokens = tokenizer_by_tense(pre_text)\n",
    "    total_word_count = len(tokens)\n",
    "    \n",
    "    # date\n",
    "    try:\n",
    "        datetime = bs.time['datetime']\n",
    "        l = re.split(r'\\s+', datetime)\n",
    "        weekday = re.sub(',','',l[0])\n",
    "        day = l[1]\n",
    "        month = l[2]\n",
    "        year = l[3]\n",
    "        time = l[4]\n",
    "        t = int(time.split(':')[0]) # 0~23\n",
    "        if t in [0,3]: \n",
    "            time_interval = 1 # 0~5\n",
    "        elif t in [4,7]: \n",
    "            time_interval = 2 # 6~11\n",
    "        elif t in [8,11]: \n",
    "            time_interval = 3 # 12~17\n",
    "        elif t in [12,15]:\n",
    "            time_interval = 4 # 18~23  \n",
    "        elif t in [16,19]:\n",
    "            time_interval = 5\n",
    "        else: \n",
    "            time_interval = 6\n",
    "    except:\n",
    "        weekday = ''\n",
    "        day = 0\n",
    "        month = ''\n",
    "        year = 0\n",
    "        time = 0\n",
    "        time_interval = 0\n",
    "    \n",
    "    # return \n",
    "    tmp = []\n",
    "    tmp.append(link_count)\n",
    "    tmp.append(img_count)\n",
    "    tmp.append(iframe_count)\n",
    "    tmp.append(quote_count)\n",
    "    tmp.append(tags)\n",
    "    tmp.append(category)\n",
    "    #tmp.append(author)\n",
    "    tmp.append(total_word_count)\n",
    "    \n",
    "    #tmp.append(parsed_date)\n",
    "    tmp.append(weekday)\n",
    "    tmp.append(day)\n",
    "    tmp.append(month)\n",
    "    tmp.append(year)\n",
    "    tmp.append(time)\n",
    "    tmp.append(time_interval)\n",
    "    \n",
    "    tmp.append(title_words_count)\n",
    "    tmp.append(title_digits_count)\n",
    "    tmp.append(sub_title_count)\n",
    "    tmp.append(title_question_mark)\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 10s, sys: 1min 14s, total: 26min 24s\n",
      "Wall time: 26min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time # about 25min\n",
    "dsize = df_train.shape[0] #df_small.shape[0]\n",
    "link_count=[]\n",
    "img_count=[]\n",
    "iframe_count=[]\n",
    "quote_count=[]\n",
    "tags=[]\n",
    "categories=[]\n",
    "authors=[]\n",
    "total_word_count=[]\n",
    "#parsed_date=[]\n",
    "weekday=[]\n",
    "day=[]\n",
    "month=[]\n",
    "year=[]\n",
    "time=[]\n",
    "time_interval=[]\n",
    "title_words_count=[]\n",
    "title_digits_count=[]\n",
    "title_question_mark=[]\n",
    "sub_title_count=[]\n",
    "\n",
    "for i in range(dsize):\n",
    "    features = extract_feature(df_train.iloc[i]['Page content'])\n",
    "    link_count.append(features[0])\n",
    "    img_count.append(features[1])\n",
    "    iframe_count.append(features[2])\n",
    "    quote_count.append(features[3])\n",
    "    tags.append(features[4])\n",
    "    categories.append(features[5])\n",
    "    #authors.append(features[6])\n",
    "    total_word_count.append(features[6])\n",
    "    #parsed_date.append(features[8])\n",
    "    weekday.append(features[7])\n",
    "    day.append(features[8])\n",
    "    month.append(features[9])\n",
    "    year.append(features[10])\n",
    "    time.append(features[11])\n",
    "    time_interval.append(features[12])\n",
    "    title_words_count.append(features[13])\n",
    "    title_digits_count.append(features[14])\n",
    "    sub_title_count.append(features[15])\n",
    "    title_question_mark.append(features[16])\n",
    "\n",
    "d = {'#link':link_count,\n",
    "     '#img':img_count,\n",
    "     '#iframe':iframe_count,\n",
    "     '#quote':quote_count,\n",
    "     'tags':tags,\n",
    "     'categories':categories,\n",
    "     #'authors':authors,\n",
    "     '#total word':total_word_count,\n",
    "     #'date':parsed_date,\n",
    "     'weekday':weekday,\n",
    "     'day':day,\n",
    "     'month':month,\n",
    "     'year':year,\n",
    "     'time':time,\n",
    "     'time interval(4hr)':time_interval,\n",
    "     '#title word':title_words_count,\n",
    "     '#title digits':title_digits_count,\n",
    "     '#sub-title':sub_title_count,\n",
    "     'If title contains \"?\"':title_question_mark,\n",
    "     'popularity':df_train['Popularity'] #\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.4 s, sys: 19.5 s, total: 58.9 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import optimized pickle written in C for serializing and \n",
    "# de-serializing a Python object\n",
    "import _pickle as pkl\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "df = DataFrame(data=d)\n",
    "\n",
    "# dump to disk\n",
    "pkl.dump(df, open('outputs/df.pkl', 'wb'))# load from disk\n",
    "#df = pkl.load(open('outputs/df.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('outputs/df.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfidf_generator(df):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "    doc = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if(i % 5000 == 0):\n",
    "            print(i)\n",
    "        try:\n",
    "            doc.append(df_train[df_train.Id == i].loc[i,'Page content'])\n",
    "        except:\n",
    "            print('Error in', i)\n",
    "    tfidf.fit(doc)\n",
    "    doc_tfidf = tfidf.transform(doc).toarray()\n",
    "    return pd.DataFrame(doc_tfidf)\n",
    "    \n",
    "#train_fea_tfidf a pandas dataframe representing the TF-IDF feature for training data\n",
    "train_fea_tfidf = tfidf_generator(df_train)\n",
    "train_fea_tfidf.head(5)\n",
    "\n",
    "test_fea_tfidf = tfidf_generator(df_test)\n",
    "test_fea_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
