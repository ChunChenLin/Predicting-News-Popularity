{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "def tokenizer_by_tense(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    important_words = []\n",
    "    for t in tagged:\n",
    "        tag_name = t[1][0:2]\n",
    "        word = t[0].lower()\n",
    "        if tag_name=='NN' or tag_name=='JJ' or tag_name=='VB' or tag_name=='RB':\n",
    "            porter = PorterStemmer()\n",
    "            important_words.append(word)\n",
    "        \n",
    "    return [porter.stem(w) for w in important_words if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "df = pd.read_csv('datasets/train.csv')\n",
    "#print(df.head(5))\n",
    "page0 = df.loc[0,'Page content']\n",
    "#print(page0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse datetime from html\n",
    "from dateutil import parser\n",
    "\n",
    "# Extract features from raw HTML\n",
    "# Usage: \n",
    "# page0 = df.loc[0,'Page content']\n",
    "# extract_feature(page0)\n",
    "def extract_feature(html):\n",
    "    \n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # link\n",
    "    link_count = len(bs.findAll('a', href=True)) \n",
    "    \n",
    "    # image\n",
    "    img_count = len(bs.findAll('img'))\n",
    "    \n",
    "    # animation, chart or video\n",
    "    iframe_count = len(bs.findAll('iframe'))\n",
    "    \n",
    "    # quote\n",
    "    quote_count = len(bs.findAll('blockquote'))\n",
    "    \n",
    "    # tags\n",
    "    tags = [] \n",
    "    for tag in bs.select('footer a'):\n",
    "        tags.append(tag.string)\n",
    "        \n",
    "    # category\n",
    "    category = \"\"\n",
    "    for cat in bs.findAll('article'):\n",
    "        try:\n",
    "            category = cat['data-channel']\n",
    "            break\n",
    "        except:\n",
    "            category = \"\"\n",
    "            continue\n",
    "            \n",
    "    # author\n",
    "    author_raw = bs.find(\"div\", { \"class\" : \"article-info\" })\n",
    "    try:\n",
    "        author = author_raw.find('a')['href']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        author = author_raw.find('span',{'class':'author_name'}).string\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # get word set\n",
    "    pre_text = preprocessor(html)\n",
    "    tokens = tokenizer_by_tense(pre_text)\n",
    "    total_word_count = len(tokens)\n",
    "    \n",
    "    # date    \n",
    "    date = bs.find('time').string\n",
    "    try:\n",
    "        parsed_date = parser.parse(date)\n",
    "    # a very little fraction of data has broken time tag\n",
    "    except:\n",
    "        parsed_date = datetime.datetime.fromordinal(735305) #random\n",
    "        #print('NO DATE')\n",
    "    publish_date = parsed_date.toordinal()\n",
    "    # publish_time = parsed_date.hour*24*60 + parsed_date.minute*60 + parsed_date.second\n",
    "    \n",
    "    print(\"link count: \",link_count)\n",
    "    print(\"image count: \",img_count)\n",
    "    print(\"iframe count: \",iframe_count)\n",
    "    print(\"quote count: \",quote_count)\n",
    "    print(\"tags: \",tags)\n",
    "    print(\"category: \",category)\n",
    "    print(\"author: \",author)\n",
    "    print(\"total word count:\", total_word_count)\n",
    "    print(\"Date: \",parsed_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link count:  22\n",
      "image count:  1\n",
      "iframe count:  0\n",
      "quote count:  0\n",
      "tags:  ['Asteroid', 'Asteroids', 'challenge', 'Earth', 'Space', 'U.S.', 'World']\n",
      "category:  world\n",
      "author:  /publishers/space-com/\n",
      "total word count: 351\n",
      "Date:  2013-06-19 15:04:30+00:00\n"
     ]
    }
   ],
   "source": [
    "extract_feature(page0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
